## Reading Ilya's list of AI papers

* [1993 - Keeping Neural Networks Simple By Minimizing the Description Length of the Weights](#keeping-neural-networks-simple-by-minimizing-the-description-length-of-the-weights)
* [2004 - A Tutorial Introduction to the Minimum Description Length Principle](#a-tutorial-introduction-to-the-minimum-description-length-principle)
* [2008 - Machine Super Intelligence](#machine-super-intelligence)
* [2011 - The First Law of Complexodynamics](#the-first-law-of-complexodynamics)
* [2012 - ImageNet Classification with Deep Convolutional Neural Networks](#imagenet-classification-with-deep-convolutional-neural-networks)
* [2014 - Neural Turing Machines](#neural-turing-machines)
* [2014 - Quantifying the Rise and Fall of Complexity in Closed Systems](#quantifying-the-rise-and-fall-of-complexity-in-closed-systems)
* [2015 - Deep Residual Learning for Image Recognition](#deep-residual-learning-for-image-recognition)
* [2015 - Neural Machine Translation by Jointly Learning to Align and Translate](#neural-machine-translation-by-jointly-learning-to-align-and-translate)
* [2015 - Recurrent Neural Network Regularization](#recurrent-neural-network-regularization)
* [2015 - The Unreasonable Effectiveness of Recurrent Neural Networks](#the-unreasonable-effectiveness-of-recurrent-neural-networks)
* [2015 - Understanding LSTM Networks](#understanding-lstm-networks)
* [2016 - Deep Speech 2: End-to-End Speech Recognition in English and Mandarin](#deep-speech-2-end-to-end-speech-recognition-in-english-and-mandarin)
* [2016 - Identity Mappings in Deep Residual Networks](#identity-mappings-in-deep-residual-networks)
* [2016 - Multi-Scale Context Aggregation by Dilated Convolutions](#multi-scale-context-aggregation-by-dilated-convolutions)
* [2016 - Order Matters: Sequence to sequence for sets](#order-matters-sequence-to-sequence-for-sets)
* [2016 - Variational Lossy Autoencoder](#variational-lossy-autoencoder)
* [2017 - A Simple Neural Network Module for Relational Reasoning](#a-simple-neural-network-module-for-relational-reasoning)
* [2017 - Attention is All You Need](#attention-is-all-you-need)
* [2017 - Kolmogorov Complexity and Algorithmic Randomness](#kolmogorov-complexity-and-algorithmic-randomness)
* [2017 - Neural Message Passing for Quantum Chemistry](#neural-message-passing-for-quantum-chemistry)
* [2017 - Pointer Networks](#pointer-networks)
* [2018 - Relational Recurrent Neural Networks](#relational-recurrent-neural-networks)
* [2019 - GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism](#gpipe-efficient-training-of-giant-neural-networks-using-pipeline-parallelism)
* [2020 - Scaling Laws for Neural Language Models](#scaling-laws-for-neural-language-models)
* [CS231n Convolutional Neural Networks for Visual Recognition](#cs231n-convolutional-neural-networks-for-visual-recognition)

# Keeping Neural Networks Simple by Minimizing the Description Length of the Weights
*Hinton, Geoffrey E., and Drew Van Camp, 1993* [(PDF)](https://www.cs.toronto.edu/~fritz/absps/colt93.pdf)

The [Minimum Description Length Principle](https://en.wikipedia.org/wiki/Minimum_description_length) asserts that the best model of some data is the one that minimizes the sum of the length of the description of the model and the length of the data encoded using that model.

* Applies the MDL principle to neural networks to control model complexity and prevent overfitting.
* Uses weight sharing to minimize the description length of the weights: The same weight is applied to multiple 
  connections in the network, reducing the number of free parameters, and thus the model complexity.

# A Tutorial Introduction to the Minimum Description Length Principle
*Peter Gr√ºnwald, 2004* [(PDF)](https://arxiv.org/pdf/math/0406077)

A clearly-written introduction to the [Minimum Description Length Principle](https://en.wikipedia.
org/wiki/Minimum_description_length). 

* Learning as data compression: Every regularity in data may be used to compress that data, and learning can be 
  equated with finding those regularities.
* MDL as model selection: The best model is the one that minimizes the sum of the length of the description of the 
  model and the length of the data encoded using that model.


# Attention is All You Need
https://nlp.seas.harvard.edu/annotated-transformer/

# The First Law of Complexodynamics

# The Unreasonable Effectiveness of Recurrent Neural Networks

# Understanding LSTM Networks

# Recurrent Neural Network Regularization

# Pointer Networks

# ImageNet Classification with Deep Convolutional Neural Networks

# Order Matters: Sequence to sequence for sets

# GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism

# Deep Residual Learning for Image Recognition

# Multi-Scale Context Aggregation by Dilated Convolutions

# Neural Message Passing for Quantum Chemistry

# Attention is All You Need

# Neural Machine Translation by Jointly Learning to Align and Translate

# Identity Mappings in Deep Residual Networks

# A Simple Neural Network Module for Relational Reasoning

# Variational Lossy Autoencoder

# Relational Recurrent Neural Networks

# Quantifying the Rise and Fall of Complexity in Closed Systems

# Neural Turing Machines

# Deep Speech 2: End-to-End Speech Recognition in English and Mandarin

# Scaling Laws for Neural Language Models

# Machine Super Intelligence

# Kolmogorov Complexity and Algorithmic Randomness

# CS231n Convolutional Neural Networks for Visual Recognition