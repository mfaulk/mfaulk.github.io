# Deep Learning: Variational Autoencoders
In the previous post about autoencoders, we looked at their ability to compress data into a latent space and then reconstruct it with remarkable fidelity. This time, we'll look at *Variational* Autoencoders (VAEs). Like the standard autoencoder, the VAE also uses neural networks to jointly learn an encoder function and a decoder function. But rather than using those neural nets to learn arbitrary functions, the VAE learns conditional probabilities that define a generative process from input to latent representation, and then from latent representation back to the input space. As a result, not only learn efficient data representations, but also allow us to easily generate new data by sampling from the learned latent space.

Variational Autoencoders take their name from *variational inference*, a powerful technique for approximating intractable probability distributions. The core idea of variational inference is to define a family of simpler, tractable distributions, and then use optimization to choose the best approximation within that family. 

# A Probabilistic Autoencoder
VAEs have probabilistic encoders and decoders. To see what that means, let's take a Bayesian approach and assume that the observed data $x$ is derived from some latent variables $z$ through a generative process. In an image processing application for facial images, $X$ could be a high-resolution photo of a person's face, while $Z$ might be a lower-dimensional vector representing the latent features of the face, such as facial structure, expression, and skin tone. 

$$\begin{align}
&p(x,z) = p(x|z)p(z) \\
&p(x) = \int p(x|z)p(z) dz 
\end{align}$$

Given an observation $x$, the Bayesian inference problem is to find the posterior distribution $p(z \| x)$. 

$$ p(z|x) = \frac{p(x|z)p(z)}{p(x)} $$

Graphically, the generative model $p(x \| z)$ goes from the latent variables to the observation, while the inferred posterior $p(z \| x)$ takes us from the observation to the latent variables:
![Graphical Model](/assets/images/vae_graphical_model.png)

We can see the connection to autoencoders by "unrolling" this model. Given an observation, the "encoder" is now inference of the latent variables $z$, and "decoding" is conditioning on the latent variables:

![Graphical Model, Unrolled](/assets/images/vae_unrolled.png)

# An Approximation

Unfortunately, we probably can't work with $p(z \| x)$ directly. The marginal probability $p(x)$ that appears in the denominator is typically unavailable or may require exponential time to integrate over all the latent variables, i.e., 

$$ p(x) = \int p(x|z)p(z) dz $$

This is where variational inference comes in. Instead of working with the (intractable) distribution $p(z \| x)$, variational inference proposes a *variational distribution* $q( z \| x)$. You can think of $q$ as a family of distributions with parameters $\phi$, and I'll sometimes write $q( z \| x; \phi)$ to make that explicit. For example, $q$ may be the family of Gaussian distributions, with $\phi$ being the mean and variance. The goal of variational inference is to choose $\phi$ so that $q( z \| x; \phi)$ is as close to $p( z \| x)$ as possible.

 
# The "Closeness" of Distributions
In order to find a distribution that is close to $p(z \| x)$, we need to define what we mean by close. The Kullback-Leibler (KL) Divergence is an information theoretic measure of how well one distribution approximates another. For continuous distributions, it's usually written as

$$\begin{align}
D_{KL}( P || Q) &= \int P(x) \log \frac{P(x)}{Q(x)} 
\end{align}$$

where $P$ is the true distribution and $Q$ is the approximate. Note that it is not symmetric: $D_{KL}(P \|\| Q) \ne D_{KL}(Q \|\| P)$. Rearranging this makes the intuition clearer:

$$\begin{align}
D_{KL}( P || Q) &= \mathbb{E}_{x \sim P} [ \log \frac{P(x)}{Q(x)} ] \\
 &= -\mathbb{E}_{x \sim P} [ \log Q(x) ] + \mathbb{E}_{x \sim P} [ \log P(x) ] \\
 &= H(P,Q) - H(P)
\end{align}$$

where
- $H(P)$ is the entropy of $P$. It represents the uncertainty in $P$, and is the average number of bits needed to communicate a symbol drawn from the distribution. 

- $H(P,Q)$ is the cross-entropy of $Q$ relative to $P$. It represents the average number of bits needed to communicate a symbol drawn from $P$ when it is encoded with respect to $Q$. 

Together, the above equation shows that $D_{KL}(P \|\| Q)$ is the average number of *additional bits* needed communicate a symbol drawn from $P$ when it is encoded with respect to $Q$. With this intuition, it makes sense that the KL divergence is always non-negative, and that it is zero only when $P = Q$.

# An Optimization... and a little ELBO grease
Putting the pieces together: Given observations $x$, we'd like to know the (intractable) posterior distribution $p(z \| x)$. Instead, we have chosen a family of tractable distributions $Q$, and we want to find the distribution $q* \in Q$ that best approximates $P(Z \| X)$. Let's try doing this by minimizing the KL divergence,

$$ q* = \arg \min_{q \in q(z | x; \phi)} D_{KL}( p(z | x) || q(z | x; \phi))$$

Unfortunately, we will hit a few obstacles.

First, $D_{KL}( p(z \| x) \|\| q(z \| x; \phi))$ still requires us to work with the posterior $p(z \| x)$, which we are assuming is intractable. The trick here is to minimize the *Reverse* KL divergence, $D_{KL}(Q \|\| P)$ instead. 

$$ q* = \arg \min_{q \in q(z | x; \phi)} D_{KL}( q(z | x; \phi) || p(z | x) )$$


This won't give us the same optima, but we will still get something that is "close" to $p(z\|x)$ in a meaningful sense (see [1] for details). Let's see what we can do with it.


$$\begin{align}
 D_{KL}( q(z|x) || p(z|x) ) &= \int q(z |x) \log \frac{q(z|x)}{p(z | x)} \\
 &= \mathbb{E}_{z \sim q} [ \log \frac{q(z|x)}{p(z|x)} ] \\
 &= \mathbb{E}_{z \sim q} [ \log q(z|x) - \log p(z|x) ] \\
 &= \mathbb{E}_{z \sim q} [ \log q(z|x) - \log \frac{p(x|z)p(z)}{p(x)} ] \\
 &= \mathbb{E}_{z \sim q} [ \log q(z|x) - \log p(x|z) - \log p(z) + \log p(x) ] \\
 &= \mathbb{E}_{z \sim q} [ \log q(z|x) - \log p(x|z) - \log p(z)] + \log p(x)  \geq 0\\
\end{align}$$

The inequality in the last line follows directly from the fact that $D_{KL}$ is non-negative. Moving the expectation over,

$$\begin{align}
\log p(x) &\geq -\mathbb{E}_{z \sim q} [ \log q(z|x) - \log p(x|z) - \log p(z)] \\ 
&= \mathbb{E}_{z \sim q}[\log p(x|z)] - \mathbb{E}_{z \sim q} [ \log q(z|x) - \log p(z)] \\ 
&= \mathbb{E}_{z \sim q}[\log p(x|z)] - \mathbb{E}_{z \sim q} [ \frac{\log q(z|x)}{p(z)}] \\ 
&= \mathbb{E}_{z \sim q}[\log p(x|z)] - D_{KL}(q(z|x) || p(z)) \\ 
\end{align}$$

The expression on the right is known as the 

(TODO: Might be able to do this more directly by using the properties of H(P,Q) and H(P))


# References and Further Reading

1.  [An Introduction to Variational Inference](https://arxiv.org/pdf/2108.13083)
1. [Chapter 13: Approximate Inference, *Deep Learning*](https://www.deeplearningbook.org/contents/inference.html)
1.  [Autoencoding Variational Bayes](https://arxiv.org/pdf/1312.6114)





