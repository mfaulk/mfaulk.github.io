# Deep Learning: Variational Autoencoders
In the [previous post about autoencoders](https://mfaulk.github.io/2024/07/26/pytorch-ae.html), we looked at their ability to compress data into a latent space and then reconstruct it with remarkable fidelity. This time, we'll look at *Variational* Autoencoders (VAEs). Like the standard autoencoder, the VAE also uses neural networks to jointly learn an encoder function and a decoder function. But rather than using those neural nets to learn arbitrary functions, the VAE learns conditional probabilities that define a generative process from input to latent representation, and then from latent representation back to the input space. As a result, a VAE not only learns an efficient data representation, but also allows us to easily generate new data by sampling from the learned latent space.

Variational Autoencoders take their name from [Variational Inference](https://arxiv.org/abs/1601.00670), a powerful technique for approximating intractable probability distributions. The core idea of variational inference is to define a family of simpler, tractable distributions, and then use optimization to choose the best approximation within that family. 

# A Probabilistic Autoencoder
VAEs have probabilistic encoders and decoders. To see what that means, let's take a Bayesian approach and assume that the observed data $x$ is derived from some latent variables $z$ through a generative process. In an image processing application for facial images, $X$ could be a high-resolution photo of a person's face, while $Z$ might be a lower-dimensional vector representing the latent features of the face, such as facial structure, expression, and skin tone. 

$$\begin{align}
&p(x,z) = p(x|z)p(z) \\
&p(x) = \int p(x|z)p(z) dz 
\end{align}$$

Given an observation $x$, the Bayesian inference problem is to find the posterior distribution $p(z \| x)$. 

$$ p(z|x) = \frac{p(x|z)p(z)}{p(x)} $$

Graphically, the generative model $p(x \| z)$ goes from the latent variables to the observation, while the inferred posterior $p(z \| x)$ takes us from the observation to the latent variables:
![Graphical Model](/assets/images/vae_graphical_model.png)

We can see the connection to autoencoders by "unrolling" this model. Given an observation, the "encoder" is now inference of the latent variables $z$, and "decoding" is conditioning on the latent variables:

![Graphical Model, Unrolled](/assets/images/vae_unrolled.png)

# An Approximation

Unfortunately, $p(z \| x)$ tends to be hard to work with. The marginal probability $p(x)$ that appears in the denominator is typically unknown -- or it may require exponential time to integrate over all the latent variables, i.e., 

$$ p(x) = \int p(x|z)p(z) dz $$

This is where variational inference comes in. Instead of working with the (intractable) distribution $p(z \| x)$, variational inference proposes a *variational distribution* $q( z \| x)$. You can think of $q$ as a family of distributions with parameters $\phi$, and I'll sometimes write $q( z \| x; \phi)$ to make that explicit. For example, $q$ may be the family of Gaussian distributions, with $\phi$ being the mean and variance. The goal of variational inference is to choose $\phi$ so that $q( z \| x; \phi)$ is as close to $p( z \| x)$ as possible.

 
# The "Closeness" of Distributions
In order to find a distribution that is "close" to $p(z \| x)$, we need to define what we mean by close. The Kullback-Leibler (KL) Divergence is an information theoretic measure of how well one distribution approximates another. For continuous distributions, it's usually written as

$$ D_{KL}( P || Q) = \int P(x) \log \frac{P(x)}{Q(x)} \, dx= \mathbb{E}_{x \sim P}[\log \frac{P(x)}{Q(x)}]$$

where $P$ is the true distribution and $Q$ is the approximate. The intuition might be clearer if we work with entropies instead of integrals or expectations:


$$ D_{KL}( P || Q) = H(P,Q) - H(P) $$ 


where
- $H(P) = - \mathbb{E}_{x \sim P} [ \log Q(x) ]$ is the entropy of $P$. It represents the uncertainty in $P$, and is the average number of bits needed to communicate a symbol drawn from the distribution. 

- $H(P,Q) = - \mathbb{E}_{x \sim P} [ \log P(x) ]$ is the cross-entropy of $Q$ relative to $P$. It represents the average number of bits needed to communicate a symbol drawn from $P$ when it is encoded with respect to $Q$. 

Together, the above equation shows that $D_{KL}(P \|\| Q)$ is the average number of *additional bits* needed communicate a symbol drawn from $P$ when it is encoded with respect to $Q$. With this intuition, it makes sense that the KL divergence is always non-negative, and that it is zero only when $P = Q$. Note that $D_{KL}$ is not symmetric: $D_{KL}(P \|\| Q) \ne D_{KL}(Q \|\| P)$. 

# An Optimization
Putting the pieces together: Given observations $x$, we'd like to know the (intractable) posterior distribution $p(z \| x)$. Instead, we have chosen a family of tractable distributions $Q$, and we want to find the distribution $q* \in Q$ that best approximates $P(Z \| X)$. Let's try doing this by minimizing the KL divergence,

$$ q* = \arg \min_{q \in q(z | x; \phi)} D_{KL}( p(z | x) || q(z | x; \phi))$$

Unfortunately, $D_{KL}( p(z \| x) \|\| q(z \| x; \phi))$ still requires us to take expectations with respect to the posterior $p(z \| x)$, which we are assuming is intractable. Instead, we'll minimize the *Reverse* KL divergence, $D_{KL}(Q \|\| P)$, 

$$ q* = \arg \min_{q \in q(z | x; \phi)} D_{KL}( q(z | x; \phi) || p(z | x) )$$


This won't give us the same optima, but we will still get something that is "close" to $p(z\|x)$ in a meaningful sense (see [1] for details). Let's see what we can do with a little, ahem, elbow grease:


TODO: introduce parameters $\theta$ for $p_{\theta}$.

$$\begin{align}
 D_{KL}( q(z|x) || p(z|x) ) &= \int q(z |x) \log \frac{q(z|x)}{p(z | x)} \\
 &= \mathbb{E}_{z \sim q} [ \log \frac{q(z|x)}{p(z|x)} ] \\
 &= \mathbb{E}_{z \sim q} [ \log q(z|x) - \log p(z|x) ] \\
 &= \mathbb{E}_{z \sim q} [ \log q(z|x) - \log \frac{p(x|z)p(z)}{p(x)} ] \\
 &= \mathbb{E}_{z \sim q} [ \log q(z|x) - \log p(x|z) - \log p(z) + \log p(x) ] \\
 &= \mathbb{E}_{z \sim q} [ \log q(z|x) - \log p(x|z) - \log p(z)] + \log p(x)  \geq 0\\
\end{align}$$

The inequality in the last line follows directly from the fact that $D_{KL}$ is non-negative. Moving the expectation over to the right,

$$\begin{align}
\log p(x) &\geq -\mathbb{E}_{z \sim q} [ \log q(z|x) - \log p(x|z) - \log p(z)] \\ 
&= \mathbb{E}_{z \sim q}[\log p(x|z)] - \mathbb{E}_{z \sim q} [ \log q(z|x) - \log p(z)] \\ 
&= \mathbb{E}_{z \sim q}[\log p(x|z)] - \mathbb{E}_{z \sim q} [ \frac{\log q(z|x)}{p(z)}] \\ 
&= \mathbb{E}_{z \sim q}[\log p(x|z)] - D_{KL}(q(z|x) || p(z)) \\ 
\end{align}$$

The expression on the right is a lower bound of the log likelihood, known as the Evidence Lower BOund, or ELBO. The model $(\theta, \phi)$ is learned by maximizing this lower bound.

TODO:
- Auto-encoding Variational Bayes has a concise explanation
- Explain why minimizing $D_{KL}(q \|\| p)$ to maximizing ELBO.
- Interpret as reconstruction loss and regularization
- Might be able to do this more directly by using the properties of H(P,Q) and H(P)

# A Variational Autoencoder
In this section, we will use neural network layers to learn the probabilistic encoder and decoder. The encoder will learn the approximate posterior $q(z \| x; \phi)$, and the decoder will learn the generative model $p(x \| z; \theta)$. We will choose the form of the variational distribution $q(z \| x; \phi)$, .... , and train the VAE on the MNIST dataset using PyTorch.

The prior over the latent variables is chosen as the isotropic multivariate Gaussian:

$$\begin{align}
p(z; \theta) &= \mathcal{N}(z; 0, I) \\
p(x | z; \theta) &= \mathcal{N}(x; \mu(z), \Sigma(z)) \\
q(z | x; \phi) &= \mathcal{N}(z; \mu, \sigma^2 I)
\end{align}$$







![Variational Autoencoder](/assets/images/vae.png)

### The Variational Distribution
- Define the variational distribution: Gaussians
- In practice, the variational distribution is chosen from the exponential family, because...
- Part of the magic is that we **don't** need to choose a more complex distribution for the latent variables. We are choosing a very simple structure for the latent variables, and are relying on the flexibility of the encoder layers and decoder layers to find a meaningful mapping to and from the latent variables. I'm sort of thinking about the latent variables as control knobs, where the knobs themselves are simple, but the machinery they control is sophisticated. 

### The "Reparametrization" Trick: The Loss Function is Differentiable
- Run input through encoder to get (sample from?) $q(z \| x; \phi)$,
- Sample z using reparametrization trick, $\epsilon \sim \mathcal{N}(0,I)$, $z = \mu_{z\|x} + \epsilon \odot \Sigma_{z\|x}$,
- Run sample through decoder to get (sample from?) $p(x \| z ; \theta)$

### Training with SGD

### Evaluation


# References and Further Reading

1.  [An Introduction to Variational Inference](https://arxiv.org/pdf/2108.13083)
1.  [Autoencoding Variational Bayes](https://arxiv.org/pdf/1312.6114)
1. [Chapter 13: Approximate Inference, *Deep Learning*](https://www.deeplearningbook.org/contents/inference.html)
1. [PyTorch VAE example](https://github.com/pytorch/examples/blob/main/vae/main.py)





