# Deep Learning: Variational Autoencoders
In the [previous post about autoencoders](https://mfaulk.github.io/2024/07/26/pytorch-ae.html), we looked at their ability to compress data into a latent space and then reconstruct it with remarkable fidelity. This time, we'll look at *Variational* Autoencoders (VAEs). 

Like the standard autoencoder, the VAE uses neural networks to jointly learn an encoder function and a decoder function. But rather than using those neural nets to learn arbitrary functions, the VAE learns (the parameters of) conditional densities that define a generative process from input to latent representation, and then from latent representation back to the input space. As a result, a VAE not only learns an efficient data representation, but also allows us to easily generate new, realistic-looking data by sampling from the learned latent space.

This post largely follows the paper [Autoencoding Variational Bayes](https://arxiv.org/pdf/1312.6114).

# A Probabilistic Autoencoder
VAEs have probabilistic encoders and decoders. To see what that means, let's take a Bayesian approach and assume that an observation $x$ is generated by a latent variable $z$:

$$\begin{align}
z_i &\sim p(z_i) \\
x_i | z_i &\sim p(x_i | z_i; \theta)
\end{align}$$

 In an image processing application for facial images, $x$ could be a high-resolution photo of a person's face, while $z$ might be a lower-dimensional vector representing the latent features of the face, such as facial structure, expression, and skin tone.  Given an observation $x$, the Bayesian inference problem is to find the posterior distribution $p(z \| x)$. 

$$ p(z|x) = \frac{p(x|z)p(z)}{p(x)} $$

Graphically, the generative model $p(x \| z)$ goes from the latent variables to the observation, while the inferred posterior $p(z \| x)$ takes us from the observation to the latent variables:
![Graphical Model](/assets/images/vae_graphical_model.png)

We can see the connection to autoencoders by "unrolling" this model. Given an observation, the "encoder" is now inference of the latent variables $z$, and "decoding" is conditioning on the latent variables:

![Graphical Model, Unrolled](/assets/images/vae_unrolled.png)

# An Approximation

Unfortunately, $p(z \| x)$ tends to be hard to work with. The marginal probability $p(x)$ that appears in the denominator is typically unknown -- or it may require exponential time to integrate over all the latent variables, i.e., 

$$ p(x) = \int p(x|z)p(z) dz $$

Variational Autoencoders take their name from [Variational Inference](https://arxiv.org/abs/1601.00670), a powerful technique for approximating intractable probability distributions. The core idea of variational inference is to define a family of simpler, tractable distributions, and then use optimization to choose the best approximation within that family. 

Instead of working with the (intractable) distribution $p(z \| x)$, variational inference proposes a *variational distribution* $q( z \| x)$. You can think of $q$ as a family of distributions with parameters $\phi$, and I'll sometimes write $q( z \| x; \phi)$ to make that explicit. For example, $q$ may be the family of Gaussian distributions, with $\phi$ being the mean and variance. The goal of variational inference is to choose $\phi$ so that $q( z \| x; \phi)$ is as close to $p( z \| x)$ as possible.

 
# The "Closeness" of Distributions
In order to find a distribution that is "close" to $p(z \| x)$, we need to define what we mean by close. The [Kullback-Leibler (KL) Divergence](https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=705c2b3e209a568f2b0b8c38795049b1e7194543) is an information theoretic measure of how well one distribution approximates another. For continuous distributions, it's usually written as

$$\begin{align}

D_{KL}( P || Q) &= \int P(x) \log \frac{P(x)}{Q(x)} \, dx \\

&= \mathbb{E}_{x \sim P} \Big[ \log \frac{P(x)}{Q(x)} \Big]

\end{align}$$

where $P$ is the true distribution and $Q$ is the approximate. The intuition is clearer if we work with entropies instead of integrals or expectations:


$$ D_{KL}( P || Q) = H(P,Q) - H(P) $$ 


where
- $H(P) = - \mathbb{E}_{x \sim P} [ \log P(x) ]$ is the entropy of $P$. It represents the uncertainty in $P$, and is the average number of bits needed to communicate a symbol drawn from the distribution. 

- $H(P,Q) = - \mathbb{E}_{x \sim P} [ \log Q(x) ]$ is the cross-entropy of $Q$ relative to $P$. It represents the average number of bits needed to communicate a symbol drawn from $P$ when it is encoded with respect to $Q$. 

Together, the above equation shows that $D_{KL}(P \|\| Q)$ is the average number of *additional bits* needed communicate a symbol drawn from $P$ when it is encoded with respect to $Q$. With this intuition, it makes sense that the KL divergence is always non-negative, and that it is zero only when $P = Q$. Note that $D_{KL}$ is not symmetric: $D_{KL}(P \|\| Q) \ne D_{KL}(Q \|\| P)$. 

# An Optimization
Putting the pieces together: Given observations $x$, we'd like to know the (intractable) posterior distribution $p(z \| x; \theta)$. Instead, we have chosen a family of tractable distributions $Q$, and we want to find the distribution $q* \in Q$ that best approximates $P(Z \| X)$. Let's try doing this by minimizing the KL divergence,

$$ q* = \arg \min_{q \in q(z | x; \phi)} D_{KL} \Big( p(z | x) || q(z | x; \phi) \Big)$$

Unfortunately, $D_{KL}( p(z \| x) \|\| q(z \| x; \phi))$ still requires us to take expectations with respect to the posterior $p(z \| x)$, which we are assuming is intractable. Instead, we'll minimize the *Reverse* KL divergence, $D_{KL}(Q \|\| P)$, 

$$ q* = \arg \min_{q \in q(z | x; \phi)} D_{KL} \Big( q(z | x; \phi) || p(z | x) \Big) $$


This won't give us the same optima, but we will still get something that is "close" to $p(z\|x)$ in a meaningful sense (see [1] for details). Let's see what we can do with it. To reduce clutter, I'm going to write $q(z \| x)$ as just $q$ for now,

$$\begin{align}
 D_{KL} \Big( q || p(z|x) \Big) &= H (q, p(z|x)) - H ( q) \\

&= H (q, p(x|z) ) + H(q, p(z) ) - H(q, p(x) ) - H( q) \\

&= H (q, p(x|z) ) + D_{KL} ( q || p(z))  - H(q , p(x) ) \\

&= H (q, p(x|z) ) + D_{KL} ( q || p(z))  + \log p(x) \geq 0

\end{align}$$

The first step uses Bayes' rule and substitutes $p(z\|x) = p(x\|z)p(z) / p(x)$ into the formula for cross-entropy. The second step regroups terms into $D_{KL} ( q(z\|x) \|\| p(z))$. The third step uses the fact that for the given x, $p(x)$ is constant. The inequality in the last line follows directly from the fact that $D_{KL}$ is non-negative. 

Rearranging gives us a nice lower bound on the log likelihood:

$$\begin{align}
\log p(x) &\geq  -H (q(z|x), p(x|z) ) - D_{KL} ( q(z|x) || p(z))
\end{align}$$

This is known as the Evidence Lower Bound, or ELBO. The cross-entropy term can be interpreted as a "reconstruction loss": given $x$, it is the expected number of additional bits needed to communicate that x, when when we sample $z \sim q(z \| x)$ and encode $x$ w.r.t $p(x\|z)$. The KL-divergence term can be viewed as a regularizer that encourages the approximate posterior $q(z\|x)$ to be close to the prior $p(z)$.

The model $(\theta, \phi)$ is learned by maximizing this lower bound.

# A Variational Autoencoder

In this section, we will use neural network layers to learn the probabilistic encoder and decoder. The encoder will learn the approximate posterior $q(z \| x; \phi)$, and the decoder will learn the generative model $p(x \| z; \theta)$. We will choose the form of the variational distribution $q(z \| x; \phi)$, .... , and train the VAE on the MNIST dataset using PyTorch.

![Variational Autoencoder](/assets/images/vae.png)

Following [2], we assume Gaussian distributions:
- $p(z) = \mathcal{N}(z; 0, I)$
- $ p(x \| z; \theta) = \mathcal{N}(x; \mu_x(z), \Sigma_x(z))$ 
- $ q(z \| x; \phi) = \mathcal{N}(z; \mu_z(x), \sigma_z(x)^2 I) $

TODO: Should the decoder directly emit $x'$, or should it emit the parameters of $p(x\|z; \theta)$?

We will use the encoder neural network to learn the functions $\mu_z(x)$ and $\sigma_z^2$, and the decoder neural network will learn the functions $\mu_x(z)$, $\Sigma_x(z)$.



```python
class VariationalAutoencoder(nn.Module):
    def __init__(self, input_size: int, code_size: int) -> None:
        """
        Variational Autoencoder (VAE).

        :param input_size: Dimension of input data x.
        :param code_size: Dimension of latent code z.
        """
        super(VariationalAutoencoder, self).__init__()
        # Encoder
        encoder_layers: List[nn.Module] = [
            nn.Linear(input_size, input_size),
            nn.ReLU(),
            # Output mu and log(sigma^2) of the latent code.
            # log(sigma^2) is used instead of sigma^2 to avoid possible numerical 
            # issues with small values.
            nn.Linear(input_size, code_size * 2),
        ]

        self.encoder = nn.Sequential(*encoder_layers)

        # Decoder
        decoder_layers: List[nn.Module] = [
            nn.Linear(code_size, input_size),
            nn.ReLU(),
            nn.Linear(input_size, input_size),
            # Pixel outputs must be in the range [0, 1].
            nn.Sigmoid(),
        ]
        self.decoder = nn.Sequential(*decoder_layers)

    def forward(self, x: Tensor) -> Tuple[Tensor, Tensor]:
        """
        Forward pass of the VAE model.

        :param x: Input tensor of shape (batch_size, input_size)
        :return: Output, latent code
        """

        # Encode the input data x into the mean and log of the variance of the latent code.
        mu, log_variance = self.encoder.chunk(2, dim=1)

        # log(sigma^2) / 2 = log(sigma), sigma = exp(log(sigma))
        sigma = torch.exp(0.5 * log_variance)
        eps = torch.randn_like(sigma)

        # Sample z|x ~ N(mu, sigma^2)
        z = mu + eps * sigma

        # Decode the latent code z.
        x_prime = self.decoder(z)

        return x_prime, z
```


### Loss Function

Returning to our objective function,

$$\begin{align}
\log p(x) &\geq  -H (q(z|x), p(x|z) ) - D_{KL} ( q(z|x) || p(z))
\end{align}$$

In some cases, the KL divergence term can be integrated analytically. That's the case here, and is actually the case whenever $q(z\|x)$ and $p(z)$ are from the exponential family.

![KL Divergence](/assets/images/kl-divergence-analytic.png)


- MCMC estimate of the cross-entropy term

$$ \mathcal{L}(\theta, \phi, x^{(i)}) \simeq 
    \frac{1}{2} \sum_{j=1}^J \Big( 
        1 + \log((\sigma_j^{(i)})^2) -(\mu_j^{(i)})^2 -(\sigma_j{(i)})^2 
    \Big) 
    + \frac{1}{L} \sum_{l=1}^L \log p_{\theta}(x^{(i)} | z^{(i,l)})$$

### Training with SGD

### Evaluation

### VAE and the exponential family
In practice, the variational distribution is chosen from the exponential family, because...

Part of the magic is that we **don't** need to choose a more complex distribution for the latent variables. We are choosing a very simple structure for the latent variables, and are relying on the flexibility of the encoder layers and decoder layers to find a meaningful mapping to and from the latent variables. I'm sort of thinking about the latent variables as control knobs, where the knobs themselves are simple, but the machinery they control is sophisticated. 

# References and Further Reading

1.  [An Introduction to Variational Inference](https://arxiv.org/pdf/2108.13083)
1.  [Autoencoding Variational Bayes](https://arxiv.org/pdf/1312.6114)
1. [Chapter 13: Approximate Inference, *Deep Learning*](https://www.deeplearningbook.org/contents/inference.html)
1. [PyTorch VAE example](https://github.com/pytorch/examples/blob/main/vae/main.py)





